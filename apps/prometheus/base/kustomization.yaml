apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: monitoring
helmCharts:
- name: prometheus
  releaseName: prometheus
  version: 19.2.0
  repo: https://prometheus-community.github.io/helm-charts
  includeCRDs: true
  valuesInline:
    alertmanager:
      extraSecretMounts:
        - name: slack-api-url
          mountPath: /etc/secrets/
          # subPath: "slack_api_url_file.txt"
          secretName: alertmanager-secret
          readOnly: true
      config:
        global:
          resolve_timeout: 1m
          slack_api_url_file: '/etc/secrets/SLACK-TOKEN'
        route:
          # A default receiver
          receiver: 'slack-notifications'
          # When a new group of alerts is created by an incoming alert, wait at
          # least 'group_wait' to send the initial notification.
          # This way ensures that you get multiple alerts for the same group that start
          # firing shortly after another are batched together on the first
          # notification.
          group_wait: 30s

          # When the first notification was sent, wait 'group_interval' to send a batch
          # of new alerts that started firing for that group.
          group_interval: 5m

          # If an alert has successfully been sent, wait 'repeat_interval' to
          # resend them.
          repeat_interval: 8h
          group_by: [ alertname, job]
        receivers:
        - name: 'slack-notifications'
          slack_configs:
          - channel: '#prometheus-notifications'
            send_resolved: true
            color: '{{ template "slack.color" . }}'
            title: '{{ template "slack.title" . }}'
            text: '{{ template "slack.text" . }}'
            actions:
              - type: button
                text: 'Runbook :green_book:'
                url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
              - type: button
                text: 'Query :mag:'
                url: '{{ (index .Alerts 0).GeneratorURL }}'
              - type: button
                text: 'Dashboard :chart_with_upwards_trend:'
                url: '{{ (index .Alerts 0).Annotations.dashboard_url }}'
              - type: button
                text: 'Silence :no_bell:'
                url: '{{ template "__alert_silence_link" . }}'

      templates:
        alertmanager.tmpl: |-
          {{/* Alertmanager Silence link */}}
          {{ define "__alert_silence_link" -}}
              {{ .ExternalURL }}/#/silences/new?filter=%7B
              {{- range .CommonLabels.SortedPairs -}}
                  {{- if ne .Name "alertname" -}}
                      {{- .Name }}%3D"{{- .Value -}}"%2C%20
                  {{- end -}}
              {{- end -}}
              alertname%3D"{{- .CommonLabels.alertname -}}"%7D
          {{- end }}

          {{/* Severity of the alert */}}
          {{ define "__alert_severity" -}}
              {{- if eq .CommonLabels.severity "critical" -}}
              *Severity:* `Critical`
              {{- else if eq .CommonLabels.severity "warning" -}}
              *Severity:* `Warning`
              {{- else if eq .CommonLabels.severity "info" -}}
              *Severity:* `Info`
              {{- else -}}
              *Severity:* :question: {{ .CommonLabels.severity }}
              {{- end }}
          {{- end }}

          {{/* Title of the Slack alert */}}
          {{ define "slack.title" -}}
            [{{ .Status | toUpper -}}
            {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
            ] {{ .CommonLabels.alertname }}
          {{- end }}


          {{/* Color of Slack attachment (appears as line next to alert )*/}}
          {{ define "slack.color" -}}
              {{ if eq .Status "firing" -}}
                  {{ if eq .CommonLabels.severity "warning" -}}
                      warning
                  {{- else if eq .CommonLabels.severity "critical" -}}
                      danger
                  {{- else -}}
                      #439FE0
                  {{- end -}}
              {{ else -}}
              good
              {{- end }}
          {{- end }}

          {{/* The text to display in the alert */}}
          {{ define "slack.text" -}}

              {{ template "__alert_severity" . }}
              {{- if (index .Alerts 0).Annotations.summary }}
              {{- "\n" -}}
              *Summary:* {{ (index .Alerts 0).Annotations.summary }}
              {{- end }}

              {{ range .Alerts }}

                  {{- if .Annotations.description }}
                  {{- "\n" -}}
                  {{ .Annotations.description }}
                  {{- "\n" -}}
                  {{- end }}
                  {{- if .Annotations.message }}
                  {{- "\n" -}}
                  {{ .Annotations.message }}
                  {{- "\n" -}}
                  {{- end }}

              {{- end }}

          {{- end }}

    server:
      alertmanagers:
        - scheme: http
          static_configs:
          - targets:
            - "prometheus-alertmanager.monitoring:9093"
    serverFiles:
      alerting_rules.yml:
        groups:
        #### Prometheus Alerts
        - name: PrometheusSelfMonitoring
          rules:
            - alert: PrometheusJobMissing
              expr: 'absent(up{job="prometheus"})'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus job missing (instance {{ $labels.instance }})
                description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetMissing
              expr: 'up == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target missing (instance {{ $labels.instance }})
                description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAllTargetsMissing
              expr: 'sum by (job) (up) == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus all targets missing (instance {{ $labels.instance }})
                description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetMissingWithWarmupTime
              expr: 'sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
                description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusConfigurationReloadFailure
              expr: 'prometheus_config_last_reload_successful != 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
                description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTooManyRestarts
              expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus too many restarts (instance {{ $labels.instance }})
                description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerJobMissing
              expr: 'absent(up{job="alertmanager"})'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus AlertManager job missing (instance {{ $labels.instance }})
                description: "A Prometheus AlertManager job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerConfigurationReloadFailure
              expr: 'alertmanager_config_last_reload_successful != 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})
                description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerConfigNotSynced
              expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
                description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerE2eDeadManSwitch
              expr: 'vector(1)'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
                description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusNotConnectedToAlertmanager
              expr: 'prometheus_notifications_alertmanagers_discovered < 1'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus not connected to alertmanager (instance {{ $labels.instance }})
                description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusRuleEvaluationFailures
              expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTemplateTextExpansionFailures
              expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusRuleEvaluationSlow
              expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
                description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusNotificationsBacklog
              expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus notifications backlog (instance {{ $labels.instance }})
                description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerNotificationFailing
              expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
                description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetEmpty
              expr: 'prometheus_sd_discovered_targets == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target empty (instance {{ $labels.instance }})
                description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetScrapingSlow
              expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus target scraping slow (instance {{ $labels.instance }})
                description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusLargeScrape
              expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus large scrape (instance {{ $labels.instance }})
                description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetScrapeDuplicate
              expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus target scrape duplicate (instance {{ $labels.instance }})
                description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCheckpointCreationFailures
              expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCheckpointDeletionFailures
              expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCompactionsFailed
              expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB compactions failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbHeadTruncationsFailed
              expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbReloadFailures
              expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB reload failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbWalCorruptions
              expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbWalTruncationsFailed
              expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTimeserieCardinality
              expr: 'label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus timeserie cardinality (instance {{ $labels.instance }})
                description: "The \"{{ $labels.name }}\" timeserie cardinality is getting very high: {{ $value }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        #### Istio Alerts
        - name: Istio
          rules:
            - alert: IstioKubernetesGatewayAvailabilityDrop
              expr: 'min(kube_deployment_status_replicas_available{deployment="istio-ingressgateway", namespace="istio-system"}) without (instance, pod) < 2'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio Kubernetes gateway availability drop (instance {{ $labels.instance }})
                description: "Gateway pods have dropped. Inbound traffic will likely be affected.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioPilotHighTotalRequestRate
              expr: 'sum(rate(pilot_xds_push_errors[1m])) / sum(rate(pilot_xds_pushes[1m])) * 100 > 5'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio Pilot high total request rate (instance {{ $labels.instance }})
                description: "Number of Istio Pilot push errors is too high (> 5%). Envoy sidecars might have outdated configuration.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioMixerPrometheusDispatchesLow
              expr: 'sum(rate(mixer_runtime_dispatches_total{adapter=~"prometheus"}[1m])) < 180'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio Mixer Prometheus dispatches low (instance {{ $labels.instance }})
                description: "Number of Mixer dispatches to Prometheus is too low. Istio metrics might not be being exported properly.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioHighTotalRequestRate
              expr: 'sum(rate(istio_requests_total{reporter="destination"}[5m])) > 1000'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Istio high total request rate (instance {{ $labels.instance }})
                description: "Global request rate in the service mesh is unusually high.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioLowTotalRequestRate
              expr: 'sum(rate(istio_requests_total{reporter="destination"}[5m])) < 100'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Istio low total request rate (instance {{ $labels.instance }})
                description: "Global request rate in the service mesh is unusually low.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioHigh4xxErrorRate
              expr: 'sum(rate(istio_requests_total{reporter="destination", response_code=~"4.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio high 4xx error rate (instance {{ $labels.instance }})
                description: "High percentage of HTTP 5xx responses in Istio (> 5%).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioHigh5xxErrorRate
              expr: 'sum(rate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio high 5xx error rate (instance {{ $labels.instance }})
                description: "High percentage of HTTP 5xx responses in Istio (> 5%).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioHighRequestLatency
              expr: 'rate(istio_request_duration_milliseconds_sum{reporter="destination"}[1m]) / rate(istio_request_duration_milliseconds_count{reporter="destination"}[1m]) > 100'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio high request latency (instance {{ $labels.instance }})
                description: "Istio average requests execution is longer than 100ms.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioLatency99Percentile
              expr: 'histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket[1m])) by (destination_canonical_service, destination_workload_namespace, source_canonical_service, source_workload_namespace, le)) > 1'
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Istio latency 99 percentile (instance {{ $labels.instance }})
                description: "Istio 1% slowest requests are longer than 1s.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: IstioPilotDuplicateEntry
              expr: 'sum(rate(pilot_duplicate_envoy_clusters{}[5m])) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Istio Pilot Duplicate Entry (instance {{ $labels.instance }})
                description: "Istio pilot duplicate entry error.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        #### ArgoCD Alerts
        - name: ArgoCD
          rules:
            - alert: ArgocdServiceNotSynced
              expr: 'argocd_app_info{sync_status!="Synced"} != 0'
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: ArgoCD service not synced (instance {{ $labels.instance }})
                description: "Service {{ $labels.name }} run by argo is currently not in sync.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ArgocdServiceUnhealthy
              expr: 'argocd_app_info{health_status!="Healthy"} != 0'
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: ArgoCD service unhealthy (instance {{ $labels.instance }})
                description: "Service {{ $labels.name }} run by argo is currently not healthy.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        #### Loki Alerts
        - name: Loki
          rules:
            - alert: LokiProcessTooManyRestarts
              expr: 'changes(process_start_time_seconds{job=~"loki"}[15m]) > 2'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Loki process too many restarts (instance {{ $labels.instance }})
                description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: LokiRequestErrors
              expr: '100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10'
              for: 15m
              labels:
                severity: critical
              annotations:
                summary: Loki request errors (instance {{ $labels.instance }})
                description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: LokiRequestPanic
              expr: 'sum(increase(loki_panic_total[10m])) by (namespace, job) > 0'
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: Loki request panic (instance {{ $labels.instance }})
                description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: LokiRequestLatency
              expr: '(histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1'
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: Loki request latency (instance {{ $labels.instance }})
                description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        #### CoreDNS Alerts
        - name: CoreDNS
          rules:
            - alert: CorednsPanicCount
              expr: 'increase(coredns_panics_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: CoreDNS Panic Count (instance {{ $labels.instance }})
                description: "Number of CoreDNS panics encountered\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        #### Promtail Alerts
        - name: Promtail
          rules:
            - alert: PromtailRequestErrors
              expr: '100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m])) by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m])) by (namespace, job, route, instance) > 10'
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: Promtail request errors (instance {{ $labels.instance }})
                description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}% errors.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PromtailRequestLatency
              expr: 'histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m])) by (le)) > 1'
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: Promtail request latency (instance {{ $labels.instance }})
                description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      prometheus.yml:
        scrape_configs:
          - job_name: serviceMonitor/argocd/argocd-metrics/0
            honor_timestamps: true
            scrape_interval: 30s
            scrape_timeout: 10s
            metrics_path: /metrics
            scheme: http
            follow_redirects: true
            relabel_configs:
            - source_labels: [job]
              separator: ;
              regex: (.*)
              target_label: __tmp_prometheus_job_name
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              separator: ;
              regex: argocd-metrics
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              separator: ;
              regex: metrics
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Node;(.*)
              target_label: node
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Pod;(.*)
              target_label: pod
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_namespace]
              separator: ;
              regex: (.*)
              target_label: namespace
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: service
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_name]
              separator: ;
              regex: (.*)
              target_label: pod
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_container_name]
              separator: ;
              regex: (.*)
              target_label: container
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: job
              replacement: ${1}
              action: replace
            - separator: ;
              regex: (.*)
              target_label: endpoint
              replacement: metrics
              action: replace
            - source_labels: [__address__]
              separator: ;
              regex: (.*)
              modulus: 1
              target_label: __tmp_hash
              replacement: $1
              action: hashmod
            - source_labels: [__tmp_hash]
              separator: ;
              regex: "0"
              replacement: $1
              action: keep
            kubernetes_sd_configs:
            - role: endpoints
              kubeconfig_file: ""
              follow_redirects: true
              namespaces:
                names:
                - argocd

          - job_name: serviceMonitor/argocd/argocd-repo-server-metrics/0
            honor_timestamps: true
            scrape_interval: 30s
            scrape_timeout: 10s
            metrics_path: /metrics
            scheme: http
            follow_redirects: true
            relabel_configs:
            - source_labels: [job]
              separator: ;
              regex: (.*)
              target_label: __tmp_prometheus_job_name
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              separator: ;
              regex: argocd-repo-server
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              separator: ;
              regex: metrics
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Node;(.*)
              target_label: node
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Pod;(.*)
              target_label: pod
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_namespace]
              separator: ;
              regex: (.*)
              target_label: namespace
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: service
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_name]
              separator: ;
              regex: (.*)
              target_label: pod
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_container_name]
              separator: ;
              regex: (.*)
              target_label: container
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: job
              replacement: ${1}
              action: replace
            - separator: ;
              regex: (.*)
              target_label: endpoint
              replacement: metrics
              action: replace
            - source_labels: [__address__]
              separator: ;
              regex: (.*)
              modulus: 1
              target_label: __tmp_hash
              replacement: $1
              action: hashmod
            - source_labels: [__tmp_hash]
              separator: ;
              regex: "0"
              replacement: $1
              action: keep
            kubernetes_sd_configs:
            - role: endpoints
              kubeconfig_file: ""
              follow_redirects: true
              namespaces:
                names:
                - argocd
          - job_name: serviceMonitor/argocd/argocd-server-metrics/0
            honor_timestamps: true
            scrape_interval: 30s
            scrape_timeout: 10s
            metrics_path: /metrics
            scheme: http
            follow_redirects: true
            relabel_configs:
            - source_labels: [job]
              separator: ;
              regex: (.*)
              target_label: __tmp_prometheus_job_name
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              separator: ;
              regex: argocd-server-metrics
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              separator: ;
              regex: metrics
              replacement: $1
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Node;(.*)
              target_label: node
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Pod;(.*)
              target_label: pod
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_namespace]
              separator: ;
              regex: (.*)
              target_label: namespace
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: service
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_name]
              separator: ;
              regex: (.*)
              target_label: pod
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_container_name]
              separator: ;
              regex: (.*)
              target_label: container
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: job
              replacement: ${1}
              action: replace
            - separator: ;
              regex: (.*)
              target_label: endpoint
              replacement: metrics
              action: replace
            - source_labels: [__address__]
              separator: ;
              regex: (.*)
              modulus: 1
              target_label: __tmp_hash
              replacement: $1
              action: hashmod
            - source_labels: [__tmp_hash]
              separator: ;
              regex: "0"
              replacement: $1
              action: keep
            kubernetes_sd_configs:
            - role: endpoints
              kubeconfig_file: ""
              follow_redirects: true
              namespaces:
                names:
                - argocd

          - job_name: 'prometheus'
            scrape_interval: 5s
            static_configs:
              - targets:
                - prometheus-server:80

          - job_name: 'alertmanager'
            scrape_interval: 10s
            honor_labels: true
            static_configs:
              - targets: ['prometheus-alertmanager:9093']

          - job_name: 'grafana'
            scrape_interval: 10s
            honor_labels: true
            static_configs:
              - targets: ['grafana:80']

          - job_name: 'node-exporter'
            kubernetes_sd_configs:
              - role: endpoints
            relabel_configs:
            - source_labels: [__meta_kubernetes_endpoints_name]
              regex: 'node-exporter'
              action: keep

          - job_name: 'kubernetes-apiservers'

            kubernetes_sd_configs:
            - role: endpoints
            scheme: https

            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

          - job_name: 'kubernetes-nodes'

            scheme: https

            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
            - role: node

            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics

          - job_name: 'kubernetes-pods'

            kubernetes_sd_configs:
            - role: pod

            relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name

          - job_name: kube-state-metrics
            honor_timestamps: true
            scrape_interval: 1m
            scrape_timeout: 1m
            metrics_path: /metrics
            scheme: http
            static_configs:
            - targets:
              - prometheus-kube-state-metrics:8080

          - job_name: 'kubernetes-cadvisor'

            scheme: https

            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
            - role: node

            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

          - job_name: 'kubernetes-service-endpoints'

            kubernetes_sd_configs:
            - role: endpoints

            relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: kubernetes_namespace

          - job_name: 'istiod'
            kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                - istio-system
            relabel_configs:
            - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: istiod;http-monitoring

          - job_name: 'envoy-stats'
            metrics_path: /stats/prometheus
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              action: keep
              regex: '.*-envoy-prom'

          - job_name: node
            static_configs:
              - targets: ['prometheus-prometheus-node-exporter:9100']